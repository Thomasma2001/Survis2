@article{9711f054e3c91c6835a3afe5c0435f3fe1f0dbf5,
title = {Low brightness PCB image enhancement algorithm for FPGA},
year = {2025},
url = {https://www.semanticscholar.org/paper/9711f054e3c91c6835a3afe5c0435f3fe1f0dbf5},
abstract = {null},
author = {Jin Han and Meijuan Zheng and Jianye Dong},
journal = {J. Real Time Image Process.},
volume = {22},
pages = {76},
doi = {10.1007/s11554-025-01635-9},
}


@Article{app14167177,
AUTHOR = {Li, Henan and Yin, Junping and Jiao, Liguo},
TITLE = {An Improved 3D Reconstruction Method for Satellite Images Based on Generative Adversarial Network Image Enhancement},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {16},
ARTICLE-NUMBER = {7177},
URL = {https://www.mdpi.com/2076-3417/14/16/7177},
ISSN = {2076-3417},
ABSTRACT = {Three-dimensional reconstruction based on optical satellite images has always been a research hotspot in the field of photogrammetry. In particular, the 3D reconstruction of building areas has provided great help for urban planning, change detection and emergency response. The results of 3D reconstruction of satellite images are greatly affected by the input images, and this paper proposes an improvement method for 3D reconstruction of satellite images based on the generative adversarial network (GAN) image enhancement. In this method, the perceptual loss function is used to optimize the network, so that it can output high-definition satellite images for 3D reconstruction, so as to improve the completeness and accuracy of the reconstructed 3D model. We use the public benchmark dataset of satellite images to test the feasibility and effectiveness of the proposed method. The experiments show that compared with the satellite stereo pipeline (S2P) method and the bundle adjustment (BA) method, the proposed method can automatically reconstruct high-quality 3D point clouds.},
DOI = {10.3390/app14167177}
}

@inproceedings{DBLP:conf/cvpr/WuLGLD24,
  author={Tsung-Han Wu and Long Lian and Joseph E. Gonzalez and Boyi Li and Trevor Darrell},
  title={Self-Correcting LLM-Controlled Diffusion Models},
  year={2024},
  cdate={1704067200000},
  pages={6327-6336},
  url={https://doi.org/10.1109/CVPR52733.2024.00605},
  booktitle={CVPR},
  crossref={conf/cvpr/2024}
}


@Article{s24041345,
AUTHOR = {Ye, Linwei and Wang, Dong and Yang, Dongyi and Ma, Zhiyuan and Zhang, Quan},
TITLE = {VELIE: A Vehicle-Based Efficient Low-Light Image Enhancement Method for Intelligent Vehicles},
JOURNAL = {Sensors},
VOLUME = {24},
YEAR = {2024},
NUMBER = {4},
ARTICLE-NUMBER = {1345},
URL = {https://www.mdpi.com/1424-8220/24/4/1345},
PubMedID = {38400503},
ISSN = {1424-8220},
ABSTRACT = {In Advanced Driving Assistance Systems (ADAS), Automated Driving Systems (ADS), and Driver Assistance Systems (DAS), RGB camera sensors are extensively utilized for object detection, semantic segmentation, and object tracking. Despite their popularity due to low costs, RGB cameras exhibit weak robustness in complex environments, particularly underperforming in low-light conditions, which raises a significant concern. To address these challenges, multi-sensor fusion systems or specialized low-light cameras have been proposed, but their high costs render them unsuitable for widespread deployment. On the other hand, improvements in post-processing algorithms offer a more economical and effective solution. However, current research in low-light image enhancement still shows substantial gaps in detail enhancement on nighttime driving datasets and is characterized by high deployment costs, failing to achieve real-time inference and edge deployment. Therefore, this paper leverages the Swin Vision Transformer combined with a gamma transformation integrated U-Net for the decoupled enhancement of initial low-light inputs, proposing a deep learning enhancement network named Vehicle-based Efficient Low-light Image Enhancement (VELIE). VELIE achieves state-of-the-art performance on various driving datasets with a processing time of only 0.19 s, significantly enhancing high-dimensional environmental perception tasks in low-light conditions.},
DOI = {10.3390/s24041345}
}



@ARTICLE{10225288,
  author={Du, Weizhi and Tian, Shihao},
  journal={Tsinghua Science and Technology}, 
  title={Transformer and GAN-Based Super-Resolution Reconstruction Network for Medical Images}, 
  year={2024},
  volume={29},
  number={1},
  pages={197-206},
  keywords={Training;PSNR;Magnetic resonance imaging;Computed tomography;Superresolution;Generative adversarial networks;Transformers;super-resolution;image reconstruction;Transformer;generative adversarial network (GAN)},
  doi={10.26599/TST.2022.9010071}}


@article{10.1145/3566125,
author = {Xu, Kang and Li, Weixin and Wang, Xia and Hu, Xiaoyan and Yan, Ke and Wang, Xiaojie and Dong, Xuan},
title = {CUR Transformer: A Convolutional Unbiased Regional Transformer for Image Denoising},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3566125},
doi = {10.1145/3566125},
abstract = {Image denoising is a fundamental problem in computer vision and multimedia computation. Non-local filters are effective for image denoising. But existing deep learning methods that use non-local computation structures are mostly designed for high-level tasks, and global self-attention is usually adopted. For the task of image denoising, they have high computational complexity and have a lot of redundant computation of uncorrelated pixels. To solve this problem and combine the marvelous advantages of non-local filter and deep learning, we propose a Convolutional Unbiased Regional (CUR) transformer. Based on the prior that, for each pixel, its similar pixels are usually spatially close, our insights are that (1) we partition the image into non-overlapped windows and perform regional self-attention to reduce the search range of each pixel, and (2) we encourage pixels across different windows to communicate with each other. Based on our insights, the CUR transformer is cascaded by a series of convolutional regional self-attention (CRSA) blocks with U-style short connections. In each CRSA block, we use convolutional layers to extract the query, key, and value features, namely Q, K, and V, of the input feature. Then, we partition the Q, K, and V features into local non-overlapped windows and perform regional self-attention within each window to obtain the output feature of this CRSA block. Among different CRSA blocks, we perform the unbiased window partition by changing the partition positions of the windows. Experimental results show that the CUR transformer outperforms the state-of-the-art methods significantly on four low-level vision tasks, including real and synthetic image denoising, JPEG compression artifact reduction, and low-light image enhancement.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = feb,
articleno = {104},
numpages = {22},
keywords = {Image denoising, non-local filters, regional self-attention, computer vision, local non-overlapped windows}
}


@InProceedings{Wan_AClinicaloriented_MICCAI2024,
        author = { Wang, Yaqi and Chen, Leqi and Hou, Qingshan and Cao, Peng and Yang, Jinzhu and Liu, Xiaoli and Zaiane, Osmar R.},
        title = { { A Clinical-oriented Lightweight Network for High-resolution Medical Image Enhancement } },
        booktitle = {proceedings of Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
        year = {2024},
        publisher = {Springer Nature Switzerland},
        volume = {LNCS 15003},
        month = {October},
        page = {3 -- 12}
}

@INPROCEEDINGS {10656638,
author = { Li, Jinlong and Li, Baolu and Tu, Zhengzhong and Liu, Xinyu and Guo, Qing and Juefei-Xu, Felix and Xu, Runsheng and Yu, Hongkai },
booktitle = { 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
title = {{ Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving }},
year = {2024},
volume = {},
ISSN = {},
pages = {15205-15215},
abstract = { Vision-centric perception systems for autonomous driving have gained considerable attention recently due to their cost-effectiveness and scalability, especially compared to LiDAR-based systems. However, these systems often struggle in low-light conditions, potentially compromising their performance and safety. To address this, our paper introduces LightDiff, a domain-tailored framework designed to enhance the low-light image quality for autonomous driving applications. Specifically, we employ a multi-condition controlled diffusion model. LightDiff works without any human-collected paired data, leveraging a dynamic data degradation process instead. It incorporates a novel multi-condition adapter that adaptively controls the input weights from different modalities, including depth maps, RGB images, and text captions, to effectively illuminate dark scenes while maintaining context consistency. Furthermore, to align the enhanced images with the detection model's knowledge, LightDiff employs perception-specific scores as rewards to guide the diffusion training process through reinforcement learning. Extensive experiments on the nuScenes datasets demonstrate that LightDiff can significantly improve the performance of several state-of-the-art 3D detectors in night-time conditions while achieving high visual quality scores, highlighting its potential to safeguard autonomous driving. },
keywords = {Image quality;Degradation;Adaptation models;Visualization;Three-dimensional displays;Vehicle detection;Reinforcement learning},
doi = {10.1109/CVPR52733.2024.01440},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52733.2024.01440},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}


@article{Zhao2024UnderwaterIE,
  title={Underwater image enhancement via adaptive white-balancing and multi-restoration image fusion},
  author={Genping Zhao and Yuanhao Xiao and Canheng Huang and Zhuowei Wang and Heng Wu},
  journal={Optical Review},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:275128192}
}

@INPROCEEDINGS{9157061,
  author={Yang, Fuzhi and Yang, Huan and Fu, Jianlong and Lu, Hongtao and Guo, Baining},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Texture Transformer Network for Image Super-Resolution}, 
  year={2020},
  volume={},
  number={},
  pages={5790-5799},
  keywords={Feature extraction;Image resolution;Task analysis;Gallium nitride;Machine learning;Indexes;Computer vision},
  doi={10.1109/CVPR42600.2020.00583}}


